HOST_IP: 10.8.32.150
NUM_NODES: 4
NUM_PROCESSES: 32
BACKEND: deepspeed
ZERO_STAGE: 3
MODEL: meta-llama/Meta-Llama-3-70B-Instruct
EXTRA_OPTS: --batch_size 1 --seq_length 512 --activation_checkpointing --profile --profile_dir /mnt/post-training-ppo/projects/z3n/prof_osdi2025_repro --compile --schedule --passes prefetch
[2024-12-16 20:08:49,018] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:49,975] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
W1216 20:08:52.842000 3456905 torch/distributed/run.py:793] 
W1216 20:08:52.842000 3456905 torch/distributed/run.py:793] *****************************************
W1216 20:08:52.842000 3456905 torch/distributed/run.py:793] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W1216 20:08:52.842000 3456905 torch/distributed/run.py:793] *****************************************
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
[2024-12-16 20:08:57,276] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:57,518] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:57,669] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:57,816] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:57,871] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:57,903] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:57,938] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:57,966] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,333] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,450] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,588] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,743] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,880] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,901] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,939] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:08:58,944] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-12-16 20:09:00,788] [INFO] [comm.py:663:init_distributed] cdb=None
[2024-12-16 20:09:00,852] [INFO] [comm.py:663:init_distributed] cdb=None
[2024-12-16 20:09:01,127] [INFO] [comm.py:663:init_distributed] cdb=None
[2024-12-16 20:09:01,256] [INFO] [comm.py:663:init_distributed] cdb=None
[2024-12-16 20:09:01,340] [INFO] [comm.py:663:init_distributed] cdb=None
[2024-12-16 20:09:01,456] [INFO] [comm.py:663:init_distributed] cdb=None
[2024-12-16 20:09:01,481] [INFO] [comm.py:663:init_distributed] cdb=None
[2024-12-16 20:09:01,481] [INFO] [comm.py:694:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-12-16 20:09:01,505] [INFO] [comm.py:663:init_distributed] cdb=None
Running on device: cuda:7 is_deepspeed: True
[2024-12-16 20:09:02,442] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Running on device: cuda:5 is_deepspeed: True
[2024-12-16 20:09:02,618] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Running on device: cuda:0 is_deepspeed: True
Loading model and tokenizer...
Running on device: cuda:4 is_deepspeed: True
Running on device: cuda:1 is_deepspeed: True
Running on device: cuda:6 is_deepspeed: True
[2024-12-16 20:09:03,116] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Running on device: cuda:2 is_deepspeed: True
[2024-12-16 20:09:03,165] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Running on device: cuda:3 is_deepspeed: True
[2024-12-16 20:09:03,226] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
[2024-12-16 20:09:03,272] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
[2024-12-16 20:09:03,309] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
[2024-12-16 20:09:03,347] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
NCCL version 2.21.5+cuda12.4
[2024-12-16 20:09:08,196] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 723, num_elems = 70.55B
Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.61it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.57it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.54it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.61it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.50it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.47it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:00<00:11,  2.43it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:19,  1.42it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:19,  1.42it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:19,  1.43it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:19,  1.41it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:19,  1.41it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:19,  1.40it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:01<00:20,  1.39it/s]Loading checkpoint shards:   3%|▎         | 1/30 [00:01<00:46,  1.60s/it]Loading checkpoint shards:  10%|█         | 3/30 [00:02<00:22,  1.20it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:02<00:22,  1.19it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:02<00:22,  1.20it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:02<00:22,  1.19it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:02<00:22,  1.19it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:02<00:22,  1.18it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:02<00:22,  1.18it/s]Loading checkpoint shards:   7%|▋         | 2/30 [00:02<00:32,  1.15s/it]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:21,  1.23it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:21,  1.24it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:21,  1.24it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:21,  1.22it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:21,  1.22it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:21,  1.23it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:03<00:21,  1.21it/s]Loading checkpoint shards:  10%|█         | 3/30 [00:03<00:27,  1.02s/it]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:20,  1.23it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:20,  1.23it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:20,  1.22it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:20,  1.22it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:20,  1.22it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:20,  1.21it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:03<00:20,  1.20it/s]Loading checkpoint shards:  13%|█▎        | 4/30 [00:04<00:25,  1.02it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:04<00:20,  1.15it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:04<00:20,  1.15it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:04<00:20,  1.15it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:04<00:20,  1.15it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:04<00:20,  1.14it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:04<00:21,  1.14it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:04<00:21,  1.13it/s]Loading checkpoint shards:  17%|█▋        | 5/30 [00:05<00:24,  1.04it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:20,  1.12it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:20,  1.12it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:20,  1.12it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:20,  1.11it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:20,  1.11it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:20,  1.11it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:05<00:21,  1.09it/s]Loading checkpoint shards:  20%|██        | 6/30 [00:06<00:22,  1.07it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:19,  1.12it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:19,  1.12it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:19,  1.12it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:19,  1.12it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:19,  1.12it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:19,  1.12it/s]Loading checkpoint shards:  23%|██▎       | 7/30 [00:06<00:20,  1.12it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:06<00:20,  1.06it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:17,  1.18it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:17,  1.18it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:17,  1.18it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:17,  1.18it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:17,  1.17it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:17,  1.17it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:07<00:17,  1.19it/s]Loading checkpoint shards:  27%|██▋       | 8/30 [00:07<00:19,  1.13it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:08<00:16,  1.20it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:08<00:16,  1.20it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:08<00:16,  1.19it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:08<00:16,  1.22it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:08<00:16,  1.19it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:08<00:16,  1.19it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:08<00:16,  1.19it/s]Loading checkpoint shards:  30%|███       | 9/30 [00:08<00:18,  1.14it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:16,  1.19it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:16,  1.19it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:16,  1.18it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:16,  1.18it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:16,  1.18it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:16,  1.17it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:09<00:16,  1.18it/s]Loading checkpoint shards:  33%|███▎      | 10/30 [00:09<00:17,  1.17it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:09<00:15,  1.19it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:10<00:14,  1.21it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:10<00:15,  1.19it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:10<00:15,  1.18it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:10<00:15,  1.18it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:10<00:15,  1.18it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:10<00:15,  1.17it/s]Loading checkpoint shards:  37%|███▋      | 11/30 [00:10<00:15,  1.20it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:10<00:14,  1.17it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:10<00:14,  1.17it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:10<00:14,  1.17it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:10<00:14,  1.18it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:10<00:14,  1.17it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:10<00:14,  1.16it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:10<00:14,  1.17it/s]Loading checkpoint shards:  40%|████      | 12/30 [00:11<00:15,  1.20it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:11<00:13,  1.22it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:11<00:13,  1.21it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:11<00:13,  1.21it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:11<00:13,  1.21it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:11<00:13,  1.21it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:11<00:13,  1.22it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:11<00:13,  1.21it/s]Loading checkpoint shards:  43%|████▎     | 13/30 [00:11<00:14,  1.19it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:12<00:12,  1.21it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:12<00:12,  1.22it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:12<00:12,  1.21it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:12<00:12,  1.20it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:12<00:12,  1.21it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:12<00:12,  1.20it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:12<00:13,  1.11it/s]Loading checkpoint shards:  47%|████▋     | 14/30 [00:12<00:13,  1.17it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:13<00:11,  1.18it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:13<00:11,  1.18it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:13<00:11,  1.19it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:13<00:11,  1.18it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:13<00:11,  1.18it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:13<00:11,  1.17it/s]Loading checkpoint shards:  50%|█████     | 15/30 [00:13<00:12,  1.17it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:13<00:12,  1.11it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:14<00:11,  1.17it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:14<00:11,  1.17it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:14<00:11,  1.17it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:14<00:11,  1.17it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:14<00:11,  1.16it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:14<00:11,  1.16it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:14<00:11,  1.15it/s]Loading checkpoint shards:  53%|█████▎    | 16/30 [00:14<00:11,  1.18it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:15<00:10,  1.16it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:15<00:10,  1.16it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:15<00:10,  1.16it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:15<00:10,  1.16it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:15<00:10,  1.16it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:15<00:10,  1.16it/s]Loading checkpoint shards:  57%|█████▋    | 17/30 [00:15<00:10,  1.19it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:15<00:10,  1.13it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:15<00:09,  1.21it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:15<00:09,  1.20it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:15<00:09,  1.20it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:15<00:09,  1.20it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:15<00:09,  1.19it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:16<00:09,  1.22it/s]Loading checkpoint shards:  60%|██████    | 18/30 [00:16<00:10,  1.19it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:16<00:09,  1.11it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:16<00:08,  1.23it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:16<00:08,  1.22it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:16<00:08,  1.22it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:16<00:08,  1.21it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:16<00:08,  1.22it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:16<00:08,  1.23it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:16<00:08,  1.18it/s]Loading checkpoint shards:  63%|██████▎   | 19/30 [00:16<00:09,  1.19it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:17<00:07,  1.21it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:17<00:07,  1.20it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:17<00:07,  1.20it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:17<00:07,  1.20it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:17<00:07,  1.21it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:17<00:07,  1.20it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:17<00:07,  1.18it/s]Loading checkpoint shards:  67%|██████▋   | 20/30 [00:17<00:08,  1.20it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:18<00:06,  1.19it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:18<00:06,  1.19it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:18<00:06,  1.19it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:18<00:06,  1.18it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:18<00:06,  1.18it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:18<00:06,  1.19it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:18<00:06,  1.17it/s]Loading checkpoint shards:  70%|███████   | 21/30 [00:18<00:07,  1.21it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:19<00:05,  1.18it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:19<00:05,  1.18it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:19<00:05,  1.18it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:19<00:05,  1.18it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:19<00:05,  1.17it/s]Loading checkpoint shards:  73%|███████▎  | 22/30 [00:19<00:06,  1.21it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:19<00:05,  1.17it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:19<00:05,  1.17it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:20<00:04,  1.22it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:20<00:04,  1.22it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:20<00:04,  1.21it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:20<00:04,  1.21it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:20<00:04,  1.21it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:20<00:04,  1.23it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:20<00:04,  1.21it/s]Loading checkpoint shards:  77%|███████▋  | 23/30 [00:20<00:05,  1.20it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:20<00:04,  1.23it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:20<00:04,  1.24it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:20<00:04,  1.23it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:20<00:04,  1.23it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:20<00:04,  1.22it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:20<00:04,  1.25it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:20<00:04,  1.22it/s]Loading checkpoint shards:  80%|████████  | 24/30 [00:21<00:05,  1.20it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:21<00:03,  1.21it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:21<00:03,  1.21it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:21<00:03,  1.21it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:21<00:03,  1.21it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:21<00:03,  1.20it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:21<00:03,  1.21it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:21<00:03,  1.21it/s]Loading checkpoint shards:  83%|████████▎ | 25/30 [00:21<00:04,  1.21it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:22<00:02,  1.20it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:22<00:02,  1.20it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:22<00:02,  1.19it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:22<00:02,  1.20it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:22<00:02,  1.20it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:22<00:02,  1.20it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:22<00:02,  1.19it/s]Loading checkpoint shards:  87%|████████▋ | 26/30 [00:22<00:03,  1.22it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:23<00:01,  1.17it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:23<00:01,  1.17it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:23<00:01,  1.16it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:23<00:01,  1.17it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:23<00:01,  1.16it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:23<00:01,  1.17it/s]Loading checkpoint shards:  90%|█████████ | 27/30 [00:23<00:02,  1.18it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:23<00:01,  1.15it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:24<00:00,  1.16it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:24<00:00,  1.15it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:24<00:00,  1.16it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:24<00:00,  1.15it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:24<00:00,  1.15it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:24<00:00,  1.18it/s]Loading checkpoint shards:  97%|█████████▋| 29/30 [00:24<00:00,  1.16it/s]Loading checkpoint shards:  93%|█████████▎| 28/30 [00:24<00:01,  1.15it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.50it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.22it/s]
Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.22it/s]
Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.22it/s]
Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.22it/s]
Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.49it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.22it/s]
Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.48it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.21it/s]
Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.40it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:24<00:00,  1.21it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading checkpoint shards:  97%|█████████▋| 29/30 [00:25<00:00,  1.17it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:25<00:00,  1.36it/s]Loading checkpoint shards: 100%|██████████| 30/30 [00:25<00:00,  1.16it/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Loading dataset...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-12-16 20:09:37,091] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-12-16 20:09:37,284] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-12-16 20:09:37,346] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
[2024-12-16 20:09:37,472] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
[2024-12-16 20:09:37,472] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-12-16 20:09:37,806] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
[2024-12-16 20:09:37,841] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
[2024-12-16 20:09:38,429] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.2+30504941, git-hash=30504941, git-branch=osdi_repro
[2024-12-16 20:09:38,429] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 32
[2024-12-16 20:09:38,449] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-12-16 20:09:38,451] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-12-16 20:09:38,451] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-12-16 20:09:38,533] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-12-16 20:09:38,533] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-12-16 20:09:38,533] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False
[2024-12-16 20:09:38,533] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer
[2024-12-16 20:09:38,762] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning
[2024-12-16 20:09:38,763] [INFO] [utils.py:782:see_memory_usage] MA 4.11 GB         Max_MA 9.92 GB         CA 11.82 GB         Max_CA 12 GB 
[2024-12-16 20:09:38,763] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 95.06 GB, percent = 5.4%
[2024-12-16 20:09:38,766] [INFO] [stage3.py:168:__init__] Reduce bucket size 500000000
[2024-12-16 20:09:38,766] [INFO] [stage3.py:169:__init__] Prefetch bucket size 50000000
[2024-12-16 20:09:38,958] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]
[2024-12-16 20:09:38,959] [INFO] [utils.py:782:see_memory_usage] MA 4.11 GB         Max_MA 4.11 GB         CA 11.82 GB         Max_CA 12 GB 
[2024-12-16 20:09:38,960] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 95.06 GB, percent = 5.4%
Parameter Offload: Total persistent parameters: 1318912 in 161 params
[2024-12-16 20:09:39,238] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]
[2024-12-16 20:09:39,239] [INFO] [utils.py:782:see_memory_usage] MA 4.11 GB         Max_MA 4.11 GB         CA 11.82 GB         Max_CA 12 GB 
[2024-12-16 20:09:39,239] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 95.06 GB, percent = 5.4%
[2024-12-16 20:09:39,447] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions
[2024-12-16 20:09:39,448] [INFO] [utils.py:782:see_memory_usage] MA 4.11 GB         Max_MA 4.11 GB         CA 11.82 GB         Max_CA 12 GB 
[2024-12-16 20:09:39,448] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 95.06 GB, percent = 5.4%
[2024-12-16 20:09:41,487] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 21
[2024-12-16 20:09:41,488] [INFO] [utils.py:782:see_memory_usage] MA 4.11 GB         Max_MA 4.11 GB         CA 4.11 GB         Max_CA 12 GB 
[2024-12-16 20:09:41,488] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 112.86 GB, percent = 6.4%
[2024-12-16 20:09:41,686] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions
[2024-12-16 20:09:41,687] [INFO] [utils.py:782:see_memory_usage] MA 4.11 GB         Max_MA 4.11 GB         CA 4.11 GB         Max_CA 4 GB 
[2024-12-16 20:09:41,687] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 109.34 GB, percent = 6.2%
[2024-12-16 20:09:41,918] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions
[2024-12-16 20:09:41,919] [INFO] [utils.py:782:see_memory_usage] MA 12.32 GB         Max_MA 12.45 GB         CA 12.76 GB         Max_CA 13 GB 
[2024-12-16 20:09:41,919] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 110.05 GB, percent = 6.2%
[2024-12-16 20:09:43,786] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-12-16 20:09:43,786] [INFO] [utils.py:782:see_memory_usage] MA 12.32 GB         Max_MA 12.32 GB         CA 12.76 GB         Max_CA 13 GB 
[2024-12-16 20:09:43,787] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 95.08 GB, percent = 5.4%
[2024-12-16 20:09:44,004] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-12-16 20:09:44,005] [INFO] [utils.py:782:see_memory_usage] MA 12.32 GB         Max_MA 12.72 GB         CA 13.16 GB         Max_CA 13 GB 
[2024-12-16 20:09:44,005] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 95.08 GB, percent = 5.4%
[2024-12-16 20:09:44,006] [INFO] [stage3.py:528:_setup_for_real_optimizer] optimizer state initialized
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
[2024-12-16 20:09:45,410] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-12-16 20:09:45,411] [INFO] [utils.py:782:see_memory_usage] MA 17.36 GB         Max_MA 21.28 GB         CA 22.11 GB         Max_CA 22 GB 
[2024-12-16 20:09:45,411] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 95.13 GB, percent = 5.4%
[2024-12-16 20:09:45,411] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3
[2024-12-16 20:09:45,411] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None
[2024-12-16 20:09:45,411] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-12-16 20:09:45,411] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-12-16 20:09:45,414] [INFO] [config.py:999:print] DeepSpeedEngine configuration:
[2024-12-16 20:09:45,414] [INFO] [config.py:1003:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   amp_enabled .................. False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   amp_params ................... False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f14ef4c83d0>
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   communication_data_type ...... None
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   disable_allgather ............ False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   dump_state ................... False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01
[2024-12-16 20:09:45,415] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   elasticity_enabled ........... False
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   fp16_enabled ................. False
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   global_rank .................. 0
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   graph_harvesting ............. False
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   loss_scale ................... 1.0
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   memory_breakdown ............. False
[2024-12-16 20:09:45,416] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False
[2024-12-16 20:09:45,417] [INFO] [config.py:1003:print]   mics_shard_size .............. -1
[2024-12-16 20:09:45,417] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   optimizer_name ............... None
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   optimizer_params ............. None
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   pld_enabled .................. False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   pld_params ................... False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   prescale_gradients ........... False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   scheduler_name ............... None
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   scheduler_params ............. None
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   sparse_attention ............. None
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   steps_per_print .............. inf
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   train_batch_size ............. 32
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   use_node_local_storage ....... False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   weight_quantization_config ... None
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   world_size ................... 32
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=100000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   zero_enabled ................. True
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True
[2024-12-16 20:09:45,418] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3
[2024-12-16 20:09:45,418] [INFO] [config.py:989:print_user_config]   json = {
    "bf16": {
        "enabled": true
    }, 
    "zero_optimization": {
        "stage": 3, 
        "sub_group_size": 1.000000e+08
    }, 
    "gradient_accumulation_steps": 1, 
    "gradient_clipping": 1.0, 
    "steps_per_print": inf, 
    "train_batch_size": 32, 
    "train_micro_batch_size_per_gpu": 1, 
    "wall_clock_breakdown": false, 
    "fp16": {
        "enabled": false
    }, 
    "zero_allow_untested_optimizer": true
}
Model prepared: <class 'deepspeed.runtime.engine.DeepSpeedEngine'>
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Emitting ninja build file /home/aiscuser/.cache/torch_extensions/py310_cu121/native_z3/build.ninja...
Building extension module native_z3...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module native_z3...
Time to load native_z3 op: 2.5248029232025146 seconds
Loading extension module native_z3...
Time to load native_z3 op: 2.558459520339966 seconds
Loading extension module native_z3...
Time to load native_z3 op: 2.5638155937194824 seconds
Loading extension module native_z3...
Time to load native_z3 op: 2.579015016555786 seconds
Loading extension module native_z3...
Loading extension module native_z3...
Loading extension module native_z3...
Time to load native_z3 op: 2.581061363220215 seconds
Time to load native_z3 op: 2.5833961963653564 seconds
Time to load native_z3 op: 2.5823488235473633 seconds
Installed CUDA version 12.3 does not match the version torch was compiled with 12.1 but since the APIs are compatible, accepting this combination
Using /home/aiscuser/.cache/torch_extensions/py310_cu121 as PyTorch extensions root...
Emitting ninja build file /home/aiscuser/.cache/torch_extensions/py310_cu121/native_z3/build.ninja...
Building extension module native_z3...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
Loading extension module native_z3...
Time to load native_z3 op: 2.450622081756592 seconds
Compiling with fast_free
Compiling with fast_free
Opt passes: [(<function schedule_prefetch at 0x7f14ef3daa70>, 0.0)]
Compiling with fast_free
Compiling with fast_free
Compiling with fast_free
Compiling with fast_free
Compiling with fast_free
Compiling with fast_free
schedule_prefetch graph_id=139719245026768 max_mem=76488966144.0 available_memory=40560295936 memory_allocated=39782224384 max_allocated=39790617088 total_param_size=2101346304 margin=0.1
size: 0, avg_duration: 0
size: 64, avg_duration: 0.00010760330042103305
size: 128, avg_duration: 0.00011558260302990675
size: 256, avg_duration: 0.00010928549454547465
size: 512, avg_duration: 0.00010827979713212699
size: 1024, avg_duration: 0.00010563470277702436
size: 2048, avg_duration: 0.00010747479973360896
size: 4096, avg_duration: 0.00010616000508889556
size: 8192, avg_duration: 0.00010754790127975866
size: 16384, avg_duration: 0.00010661400301614776
size: 32768, avg_duration: 0.00010927689436357468
size: 65536, avg_duration: 0.00011111189814982936
size: 131072, avg_duration: 0.00014174050011206418
size: 262144, avg_duration: 0.0002071783965220675
size: 524288, avg_duration: 0.00029041769448667765
size: 1048576, avg_duration: 0.00027263100491836667
size: 2097152, avg_duration: 0.00027366989525035024
size: 4194304, avg_duration: 0.00028613177710212767
size: 8388608, avg_duration: 0.0002672738046385348
size: 16777216, avg_duration: 0.0003418551932554692
size: 33554432, avg_duration: 0.0004158430965617299
size: 67108864, avg_duration: 0.000645305379293859
size: 134217728, avg_duration: 0.0009713481995277107
size: 268435456, avg_duration: 0.0017535523511469364
size: 536870912, avg_duration: 0.0031545190140604973
size: 1073741824, avg_duration: 0.005917279981076717
size: 2147483648, avg_duration: 0.01153399795293808
schedule_prefetch graph_id=139719214555120 max_mem=76488966144.0 available_memory=40560295936 memory_allocated=39790617088 max_allocated=39790617600 total_param_size=0 margin=0.1
schedule_prefetch graph_id=139719213081376 max_mem=76488966144.0 available_memory=40560295936 memory_allocated=39790617088 max_allocated=39791141376 total_param_size=0 margin=0.1
schedule_prefetch graph_id=139719221798960 max_mem=76488966144.0 available_memory=40560295936 memory_allocated=39788503552 max_allocated=40459635200 total_param_size=136904720384 margin=0.1
schedule_prefetch graph_id=139708880158000 max_mem=76488966144.0 available_memory=40090533888 memory_allocated=40460509696 max_allocated=40985334272 total_param_size=2101346304 margin=0.1
schedule_prefetch graph_id=139708880158000 max_mem=76488966144.0 available_memory=40090533888 memory_allocated=40968026112 max_allocated=40976418816 total_param_size=2101346304 margin=0.1
schedule_prefetch graph_id=139719221798960 max_mem=76488966144.0 available_memory=40354775040 memory_allocated=40451590144 max_allocated=40459978752 total_param_size=136904720384 margin=0.1
schedule_prefetch graph_id=139719245026768 max_mem=76488966144.0 available_memory=40858091520 memory_allocated=39787972608 max_allocated=39787972608 total_param_size=0 margin=0.1
Epoch 1, Step 10, Loss: 0.36861541867256165 sync: True time: 4.083237886428833 alloc_mem: 39782223872 peak_mem: 47573789184
Epoch 1, Step 20, Loss: 0.11279493570327759 sync: True time: 4.039803981781006 alloc_mem: 39782223872 peak_mem: 47573789184
Epoch 1, Step 30, Loss: 0.1484411060810089 sync: True time: 4.050072908401489 alloc_mem: 39782223872 peak_mem: 47573789184
meta-llama/Meta-Llama-3-70B-Instruct ds=True np=32 batch_size=1 seq=512 zero_stage=3 acc=1 ac=True compile=True schedule=True passes=prefetch compile_time=424.0224463939667 iteration time: 4.0511 alloc_mem: 39782223872 peak_mem: 47573789184
